# LSH_duplicate_detection

Note: the code used was written in collaboration with Sjoerd Bommer (621763)

This page contains the code of my solution to the assignment of Computer Science for Business Analytics, given at the Erasmus School of Economics. \
For the assignment, students were asked to create a scalable duplicate detection method, using Locality Sensitive Hashing. The dataset used contains web scraped information of 1624 televisions, from four different Web shops: Amazon.com, Newegg.com, Best-Buy.com, and TheNerds.net. Each TV has the following data: Shop name, Title, a set of Features, and the ModelID. As the ModelID is often not explicitly stated in the product features, the ModelID can not be used directly for the duplicate detection, and is rather used for evaluation purposes, as equal ModelIDs correspond to duplicate products.\
\
The structure of the code is as follows: 

First, the data is loaded, and pre-processed. In particular, we normalize the titles by setting everything to lower cases. Additionally, we replace all variations of inch (Inch, ", -inch, inches, etc.) by 'inch'. We do the same for variations of hz. Also, we delete all kinds of punctuation from the titles. \
Then, we use 5 bootstraps for robust results. We then do the following for a threshold t: \
First, we create product representations that are used for LSH. From the normalized titles, we include in the product representations all "Model Words", words that contain both letters and numbers. We also have a list of all TV brands, and we go through all titles word-by-word, and add any brands that we find in the title to the product representations. Additionally, we create a list of words that are very likely to be model IDs, which are obtained by doing some data cleaning on the titles. Same as the TV brands, we go through all titles word-by-word, and add any model IDs that we find in the title to the product representations. \
Then, we transform these product representations for all TVs into a binary matrix, containing all 'tokens' from the product representations as rows, and TVs as columns. This binary matrix is then transformed into a signature matrix using Minhashing. Minhashing reduces large binary matrices into smaller signature matrices using random hash functions. We use 50 random hash functions of the form (a*x + b) % P, where x is the input, a and b are (unique) random integers between 0 and 50, and P is the first prime larger than 10 * the number of TVs. This way, we ensure that each hash function is unique and that there is a sufficient number of hash buckets. To implement minhashing, we use an efficient algorithm that was proposed in the lectures. \
The resulting signature matrix is then used to accurately and quickly compare the similarity of two TVs, using LSH. The output of LSH is a list of candidate pairs, which all have the property that they have been hashed to the same bucket for at least one band. LSH divides the signature matrix M into b bands, each containing r rows. Obviously, it should hold that b * r = nRows(M), but we use an approximation such that the last band can contain less than r rows. Furthermore, in the lecture, it was derived that a similarity threshold t can be approximated by taking t = (1/b) ^ (1/r). Therefore, given t, we take b and r to satisfy this relationship as good as possible. As t is changed from 0 to 1 with steps of 0.05, we therefore go from many to 1 band, producing less and less candidate pairs. \ 
Given the candidate pairs and threshold t, we compute for the test set only, the Pair Quality, Pair Completeness, F1_star (the harmonic mean of PQ and PC), and the fraction of comparisons. Note that we also do LSH on the train set, as we use that as input for the Logistic Regression. \
We use LogisticRegression as a classification algorithm to detect true candidate pairs out of the candidate pairs. The entries are pairs of TVs, and the dependent variable indicates whether this pair of TVs is a duplicate (1) or not (0), which is constructed by comparing the true ModelIDs (which was not used for training, but remained available in the data but hidden to the models). The features used to explain and then predict true duplicates are: 'Same shop', 'Same brand', 'Model ID Pair' (whether the TVs in the pair have the same (likely) ModelID in the product representation), 'Signature Similarity' (Jaccard similarity of signature vectors), and 'Title Similarity' (3-gram Jaccard similarity of the titles). The hyperparameter C, which controls overfitting, was optimized using GridsearchCV for each bootstrap. The algorithm is trained on the training data, and tested on the test data, on which the F1-score is calculated. \
This entire procedure is repeated for 5 bootstraps per threshold level, and threshold is changed from 0 to 1 with steps of 0.05. 
